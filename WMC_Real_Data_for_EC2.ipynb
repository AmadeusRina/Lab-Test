{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAJ21dhPeUUS",
        "outputId": "0665f3a7-27a6-4033-dfd3-bac9e068e3cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MOSEK\n",
            "  Downloading mosek-11.0.16-cp39-abi3-manylinux2014_x86_64.whl.metadata (698 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from MOSEK) (2.0.2)\n",
            "Downloading mosek-11.0.16-cp39-abi3-manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: MOSEK\n",
            "Successfully installed MOSEK-11.0.16\n"
          ]
        }
      ],
      "source": [
        "!pip install MOSEK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1UxLcEgsGML"
      },
      "source": [
        "Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jOOQWpEsHRr",
        "outputId": "85dc29de-d817-46c0-ce7f-a1ea95372871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing base.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile base.py\n",
        "from typing import Tuple, List, Union\n",
        "import numpy as np\n",
        "import numpy.linalg as npl\n",
        "\n",
        "vector = Union[np.ndarray]\n",
        "entry_matrix = Tuple[int, int, float]\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "    def __init__(self,\n",
        "                 obs: List[entry_matrix],\n",
        "                 shape: Tuple[int, int]):\n",
        "\n",
        "        self.obs = obs\n",
        "        self.shape = shape\n",
        "        # self.num_observations: int = len(obs)\n",
        "\n",
        "        self.count_mat: vector = np.zeros(shape)\n",
        "        self.obs_mat: vector = np.zeros(shape)\n",
        "        self.num_observed_entries: int = 0\n",
        "\n",
        "        self.update_observations()\n",
        "\n",
        "    def update_observations(self):\n",
        "        for ob in self.obs:\n",
        "            row_idx, col_idx, value = ob\n",
        "            self.obs_mat[row_idx, col_idx] += value\n",
        "            self.count_mat[row_idx, col_idx] += 1\n",
        "        self.num_observed_entries = np.count_nonzero(self.count_mat)\n",
        "\n",
        "    def loss(self,\n",
        "             n_estimator: vector,\n",
        "             alphas: Union[float, list[float], np.ndarray],\n",
        "             weight_quad_loss: vector,\n",
        "             weight_nuc_norm: vector) -> np.ndarray:\n",
        "\n",
        "        if isinstance(alphas, float):\n",
        "            alphas = [alphas]\n",
        "\n",
        "        alphas = np.array(alphas)\n",
        "\n",
        "        nuc_norm = npl.norm(n_estimator, 'nuc')\n",
        "\n",
        "        rss = self.get_rss(n_estimator, weight_quad_loss, weight_nuc_norm)\n",
        "        reg = nuc_norm\n",
        "\n",
        "        return rss + alphas * reg\n",
        "\n",
        "    def get_rss(self,\n",
        "                n_estimator: vector,\n",
        "                weight_quad_loss: vector,\n",
        "                weight_nuc_norm: vector) -> float:\n",
        "\n",
        "        obs_loc = np.nonzero(self.count_mat)\n",
        "\n",
        "        product_1 = np.multiply(np.sqrt(self.count_mat[obs_loc]), 1./np.sqrt(weight_nuc_norm[obs_loc]))\n",
        "\n",
        "        product_2 = np.multiply(product_1, n_estimator[obs_loc])\n",
        "\n",
        "        difference = product_2 - np.multiply(self.obs_mat[obs_loc], 1./np.sqrt(self.count_mat[obs_loc]))\n",
        "        product_3 = np.multiply(np.sqrt(weight_quad_loss[obs_loc]), difference)\n",
        "\n",
        "        return 1 / (2 * self.num_observed_entries) * npl.norm(product_3) ** 2\n",
        "\n",
        "    def get_rss_grad(self,\n",
        "                     n_estimator: vector,\n",
        "                     weight_quad_loss: vector,\n",
        "                     weight_nuc_norm: vector) -> vector:\n",
        "\n",
        "        operand_1 = np.multiply(self.count_mat, np.multiply(1. / weight_nuc_norm, n_estimator))\n",
        "        operand_2 = np.multiply(1. / np.sqrt(weight_nuc_norm), self.obs_mat)\n",
        "        difference = operand_1 - operand_2\n",
        "        return (1 / self.num_observed_entries) * np.multiply(weight_quad_loss, difference)\n",
        "\n",
        "    def get_init_step_size(self,\n",
        "                           weight_quad_loss: vector,\n",
        "                           weight_nuc_norm: vector) -> float:\n",
        "        product = np.multiply(1. / np.sqrt(weight_nuc_norm), np.sqrt(self.count_mat))\n",
        "        maximum_element = np.max(np.multiply(product, np.sqrt(weight_quad_loss)))\n",
        "        init_step_size = self.num_observed_entries / (maximum_element ** 2)\n",
        "\n",
        "        return init_step_size\n",
        "\n",
        "    def get_marginal_probabilities(self):\n",
        "        row_marginal = self.count_mat.sum(axis=1)/self.count_mat.sum()\n",
        "        col_marginal = self.count_mat.sum(axis=0)/self.count_mat.sum()\n",
        "\n",
        "        return np.outer(row_marginal, col_marginal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EAZ6v1ssjXN"
      },
      "source": [
        "WMC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kN5EMpyskD1",
        "outputId": "a34ef251-528e-4aac-bc07-ed4dd29ac962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wmc.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile wmc.py\n",
        "# wmc.py  —  Weight‑Matrix Completion / NU‑Recommend solver\n",
        "# --------------------------------------------------------------------\n",
        "from typing import Optional, Tuple, List, Union, Any\n",
        "from collections import namedtuple, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import numpy.linalg as npl\n",
        "import cvxpy as cvx\n",
        "\n",
        "from base import Dataset\n",
        "\n",
        "vector = Union[np.ndarray]\n",
        "entry_matrix = Tuple[int, int, float]\n",
        "\n",
        "SolverMetrics = namedtuple('SolverMetrics',\n",
        "                           ['loss', 'difference_norm', 'old_norm'])\n",
        "\n",
        "DEFAULT_XTOL = 1e-5\n",
        "\n",
        "\n",
        "class WmcImpute:\n",
        "    \"\"\"\n",
        "    Main solver class implementing\n",
        "      • margin‑weighted trace‑norm minimisation\n",
        "      • NU‑Recommend (weight derived via Eq.(6))\n",
        "      • free/uniform baseline\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    def __init__(\n",
        "        self,\n",
        "        shape: Tuple[int, int],\n",
        "        weight_quad_loss: Optional[vector] = None,\n",
        "        weight_nuc_norm: Optional[vector] = None,\n",
        "        metrics: Optional['Metric'] = None\n",
        "    ):\n",
        "        if metrics is None:\n",
        "            metrics = Metric()\n",
        "\n",
        "        self.metrics = metrics\n",
        "        self.shape   = shape\n",
        "\n",
        "        # quadratic‑loss weights (IPW)\n",
        "        if weight_quad_loss is None:\n",
        "            self.weight_quad_loss = np.ones(shape)\n",
        "        else:\n",
        "            self.weight_quad_loss = weight_quad_loss\n",
        "\n",
        "        # nuclear‑norm weights  (default: uniform scaling 1/(d·d))\n",
        "        if weight_nuc_norm is None:\n",
        "            self.weight_nuc_norm = np.ones(shape) / (shape[0] * shape[1])\n",
        "        else:\n",
        "            self.weight_nuc_norm = weight_nuc_norm\n",
        "\n",
        "        # step‑size / continuation params\n",
        "        self.init_step_size: float = 0.0\n",
        "        self.line_search_shrinkage_amount: float = 1 / 1.2   # ← β\n",
        "\n",
        "        # iterate storage\n",
        "        self.n_old: Optional[vector] = None\n",
        "        self.n_new: Optional[vector] = None\n",
        "        self._init_n()\n",
        "\n",
        "        self.xtol: float = DEFAULT_XTOL\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    #           -----   weight construction for NU‑Recommend  -----\n",
        "    # ----------------------------------------------------------------\n",
        "    def set_weight_nuc_norm_for_nu_rec(self,\n",
        "                                       b_hat,\n",
        "                                       p_hat,\n",
        "                                       ell,\n",
        "                                       gamma):\n",
        "        \"\"\"Compute W = Q^2 via Eq.(6) and store as weight_nuc_norm.\"\"\"\n",
        "        self.weight_nuc_norm = self.compute_weight(b_hat, p_hat,\n",
        "                                                   ell, gamma)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_weight(b_hat,\n",
        "                       p_hat,\n",
        "                       ell: float,\n",
        "                       gamma: float):\n",
        "        \"\"\"\n",
        "        Solve\n",
        "            minimise    ‖ B_hat ⊙ Q ‖_*\n",
        "            subject to  sqrt(P_hat)/ℓ ≤ Q ≤ ℓ·sqrt(P_hat)\n",
        "                        ‖ B_hat ⊙ Q ‖_∞ ≤ γ\n",
        "        Return W = Q ⊙ Q.\n",
        "\n",
        "        We call MOSEK with tight tolerances so results match\n",
        "        MATLAB / CVX (≈ 1e‑8 rel‑gap).\n",
        "        \"\"\"\n",
        "        import numpy as np\n",
        "\n",
        "        # decision variable\n",
        "        n1, n2 = b_hat.shape\n",
        "        q = cvx.Variable((n1, n2))\n",
        "\n",
        "        sqrt_p = np.sqrt(p_hat)\n",
        "        BQ     = cvx.multiply(b_hat, q)\n",
        "\n",
        "        constraints = [\n",
        "            q >= cvx.multiply(sqrt_p, 1 / ell),\n",
        "            q <= cvx.multiply(sqrt_p, ell),\n",
        "            cvx.norm(BQ, \"inf\") <= gamma\n",
        "        ]\n",
        "\n",
        "        prob = cvx.Problem(cvx.Minimize(cvx.normNuc(BQ)), constraints)\n",
        "\n",
        "        mosek_tol = {\n",
        "            'MSK_DPAR_INTPNT_CO_TOL_REL_GAP': 1e-8,\n",
        "            'MSK_DPAR_INTPNT_CO_TOL_PFEAS'  : 1e-9,\n",
        "            'MSK_DPAR_INTPNT_CO_TOL_DFEAS'  : 1e-9\n",
        "        }\n",
        "\n",
        "        prob.solve(solver=cvx.MOSEK,\n",
        "                   mosek_params=mosek_tol,\n",
        "                   verbose=False)\n",
        "\n",
        "        if q.value is None:\n",
        "            raise RuntimeError(\"compute_weight: MOSEK failed — no Q.\")\n",
        "\n",
        "        return q.value ** 2   # element‑wise square → W matrix\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    #           ---------   proximal‑gradient solver   ---------\n",
        "    # ----------------------------------------------------------------\n",
        "    def _init_n(self):\n",
        "        self.n_old = None\n",
        "        self.n_new = self.zero()\n",
        "\n",
        "    def zero(self):\n",
        "        return np.zeros(self.shape)\n",
        "\n",
        "    # --------------  public API -----------------\n",
        "    def impute(self, ds: Dataset, **kwargs):\n",
        "        assert 'alpha' in kwargs\n",
        "        alpha_min = kwargs['alpha']\n",
        "\n",
        "        num_alpha = kwargs.get('num_alpha', 30)\n",
        "        alphas = self.get_alpha_seq(ds, alpha_min, num_alpha)\n",
        "        return self.fit(ds, alphas, **kwargs)[-1]\n",
        "\n",
        "    # --------------  helper routines  --------------\n",
        "    def get_alpha_seq(self, ds: Dataset,\n",
        "                      alpha_min: float,\n",
        "                      num_alpha: int) -> List[float]:\n",
        "        alpha = self.alpha_max(ds)\n",
        "        while alpha_min >= alpha:\n",
        "            alpha_min *= 0.1\n",
        "        return list(np.logspace(np.log10(alpha),\n",
        "                                np.log10(alpha_min), num_alpha))\n",
        "\n",
        "    def alpha_max(self, ds: Dataset) -> float:\n",
        "        grad = ds.get_rss_grad(self.zero(),\n",
        "                               self.weight_quad_loss,\n",
        "                               self.weight_nuc_norm)\n",
        "        return npl.norm(grad, 2)\n",
        "\n",
        "    # --------------  main fit loop  ----------------\n",
        "    def fit(self,\n",
        "            ds: Dataset,\n",
        "            alphas: List[float],\n",
        "            max_iterations: int = 5000,\n",
        "            **kwargs) -> List[np.ndarray]:\n",
        "        self._prefit(ds, **kwargs)\n",
        "        self._init_n()\n",
        "\n",
        "        bs: List[np.ndarray] = []\n",
        "        for alpha in alphas:\n",
        "            for _ in range(max_iterations):\n",
        "                metrics = self.update_once(ds, alpha)\n",
        "                if self.should_stop(metrics):\n",
        "                    break\n",
        "            bs.append(np.multiply(self.n_new,\n",
        "                                  1 / np.sqrt(self.weight_nuc_norm)))\n",
        "        return bs\n",
        "\n",
        "    def _prefit(self, ds: Dataset, **kwargs):\n",
        "        self.init_step_size = kwargs.get(\n",
        "            'init_step_size',\n",
        "            ds.get_init_step_size(self.weight_quad_loss,\n",
        "                                  self.weight_nuc_norm))\n",
        "        self.xtol = kwargs.get('xtol', DEFAULT_XTOL)\n",
        "\n",
        "    # --------------  one PGD iteration  ------------\n",
        "    def update_once(self, ds: Dataset, alpha: float) -> SolverMetrics:\n",
        "        n_old   = self.n_new\n",
        "        step_sz = self.get_step_size(ds, alpha)\n",
        "        gen_grad = self.get_generalized_grad(ds, alpha, step_sz)\n",
        "        self.n_new = n_old - step_sz * gen_grad\n",
        "\n",
        "        return SolverMetrics(\n",
        "            loss=ds.loss(self.n_new, alpha,\n",
        "                         self.weight_quad_loss, self.weight_nuc_norm),\n",
        "            difference_norm=npl.norm(n_old - self.n_new),\n",
        "            old_norm=npl.norm(n_old)\n",
        "        )\n",
        "\n",
        "\n",
        "    def get_step_size(self, ds: Dataset, alpha: float):\n",
        "        step_size = self.init_step_size\n",
        "        beta = self.line_search_shrinkage_amount\n",
        "\n",
        "        MAX_STEP_SIZE = 1e307\n",
        "\n",
        "        if self.should_backtrack(ds, alpha, step_size):\n",
        "            while self.should_backtrack(ds, alpha, step_size):\n",
        "                step_size *= beta\n",
        "                if step_size > MAX_STEP_SIZE:\n",
        "                    step_size = MAX_STEP_SIZE\n",
        "                    break\n",
        "        else:\n",
        "            while not self.should_backtrack(ds, alpha, step_size):\n",
        "                step_size /= beta\n",
        "                if step_size > MAX_STEP_SIZE:\n",
        "                    step_size = MAX_STEP_SIZE\n",
        "                    break\n",
        "            step_size *= beta\n",
        "            if step_size > MAX_STEP_SIZE:\n",
        "                step_size = MAX_STEP_SIZE\n",
        "\n",
        "        return step_size\n",
        "\n",
        "    def should_backtrack(self, ds: Dataset, alpha: float, step_size: float) -> bool:\n",
        "        rss_grad = ds.get_rss_grad(self.n_new, self.weight_quad_loss, self.weight_nuc_norm)\n",
        "        generalized_grad = self.get_generalized_grad(ds, alpha, step_size)\n",
        "\n",
        "        n_new = self.n_new\n",
        "        n_new_next = n_new - step_size * generalized_grad\n",
        "\n",
        "        rss = ds.get_rss(n_new, self.weight_quad_loss, self.weight_nuc_norm)\n",
        "        rss_next = ds.get_rss(n_new_next, self.weight_quad_loss, self.weight_nuc_norm)\n",
        "\n",
        "        term_2 = step_size * np.sum(np.multiply(rss_grad, generalized_grad))\n",
        "        term_3 = step_size/2 * (npl.norm(generalized_grad) ** 2)\n",
        "        threshold = rss - term_2 + term_3\n",
        "        return rss_next > threshold\n",
        "\n",
        "    def get_generalized_grad(self, ds: Dataset, alpha: float, step_size: float) -> np.ndarray:\n",
        "        rss_grad = ds.get_rss_grad(self.n_new, self.weight_quad_loss, self.weight_nuc_norm)\n",
        "        grad_descent = self.n_new - step_size * rss_grad\n",
        "        u, s, vh = np.linalg.svd(grad_descent, full_matrices=False)\n",
        "        s_threshold = np.maximum(s - step_size * alpha, 0)\n",
        "        numerator = self.n_new - np.dot(u * s_threshold, vh)\n",
        "        return numerator / step_size\n",
        "\n",
        "    def should_stop(self, metrics: Any) -> bool:\n",
        "        difference_norm = metrics.difference_norm\n",
        "        old_norm = metrics.old_norm\n",
        "        if difference_norm < self.xtol * max(1, old_norm):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def update_metrics(self, alphas, bs, b, p):\n",
        "        for i, alpha in enumerate(alphas):\n",
        "            b_hat = bs[i]\n",
        "            l2_error = npl.norm(np.multiply(np.sqrt(p), b_hat - b)) / npl.norm(np.multiply(np.sqrt(p), b))\n",
        "            frobenius_error = npl.norm(b_hat - b) / npl.norm(b)\n",
        "            self.metrics.add_errors(alpha, l2_error, frobenius_error)\n",
        "\n",
        "\n",
        "class Metric:\n",
        "    def __init__(self):\n",
        "        self.alphas = []\n",
        "        self.l2_errors = []\n",
        "        self.frobenius_errors = []\n",
        "        self.best_b_hat = None\n",
        "\n",
        "    def add_errors(self, alpha, l2_error, frobenius_error):\n",
        "        self.alphas.append(alpha)\n",
        "        self.l2_errors.append(l2_error)\n",
        "        self.frobenius_errors.append(frobenius_error)\n",
        "\n",
        "    def get_best_b_hat(self, bs):\n",
        "        return bs[np.argmin(self.l2_errors)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.alphas)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return defaultdict(alpha=self.alphas[i], l2_error=self.l2_errors[i], frobenius_error=self.frobenius_errors[i])\n",
        "\n",
        "    def __iter__(self):\n",
        "        def _iter():\n",
        "            for i in range(len(self)):\n",
        "                yield self[i]\n",
        "        return _iter()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mBOpIJ2swDY"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA0bT943szTU",
        "outputId": "b72bed51-80eb-40f9-a200-0e28bef6a37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "from typing import Tuple, List\n",
        "import random\n",
        "\n",
        "def check_random_state(random_state):\n",
        "    if random_state is None:\n",
        "        return npr.get_state()\n",
        "    elif isinstance(random_state, int):\n",
        "        return npr.RandomState(random_state)\n",
        "    else:\n",
        "        return random_state\n",
        "\n",
        "\n",
        "def random_state_generator(seed):\n",
        "\n",
        "    def _random_state_gen(_seed):\n",
        "        while True:\n",
        "            yield npr.RandomState(_seed)\n",
        "            _seed += 1\n",
        "\n",
        "    gen = _random_state_gen(seed)\n",
        "\n",
        "    def _next_random_state():\n",
        "        return next(gen)\n",
        "\n",
        "    return _next_random_state\n",
        "\n",
        "\n",
        "class MetricAggregator:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.m0 = 0\n",
        "        self.m1 = 0\n",
        "        self.m2 = 0\n",
        "\n",
        "    def get_confidence_band(self):\n",
        "\n",
        "        m0 = self.m0\n",
        "        m1 = self.m1\n",
        "        m2 = self.m2\n",
        "        m0 = np.maximum(m0, 1)\n",
        "\n",
        "        mean = m1 / m0\n",
        "        var = (m2 - m1 ** 2 / m0) / (m0 - 1)\n",
        "        sd = var ** 0.5\n",
        "        se = (var / m0) ** 0.5\n",
        "\n",
        "        return mean, sd, se\n",
        "\n",
        "    def aggregate(self, x):\n",
        "        self.m0 += 1\n",
        "        self.m1 += x\n",
        "        self.m2 += x ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVyw62Ips71j"
      },
      "source": [
        "Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSgSGmmHs9xb",
        "outputId": "92064d70-c72c-4ebf-cbe7-e9f85e9b1ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing experiments.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile experiments.py\n",
        "from typing import Dict\n",
        "from argparse import ArgumentParser\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from base import Dataset\n",
        "from wmc import Metric\n",
        "from utils import random_state_generator, MetricAggregator\n",
        "\n",
        "def create_free(shape):\n",
        "    from wmc import WmcImpute\n",
        "    solver = WmcImpute(shape)\n",
        "\n",
        "    return solver\n",
        "\n",
        "\n",
        "def create_margin(shape, margin_weight):\n",
        "    from wmc import WmcImpute\n",
        "    solver = WmcImpute(shape, weight_nuc_norm=margin_weight)\n",
        "    return solver\n",
        "\n",
        "\n",
        "def create_nu_recommend(shape):\n",
        "    from wmc import WmcImpute\n",
        "    solver = WmcImpute(shape)\n",
        "\n",
        "    return solver\n",
        "\n",
        "\n",
        "def run_once(run_seed, run, shape, r, sd, num_samples, next_random_state, ell, gamma) -> Dict[str, Metric]:\n",
        "\n",
        "    output_dir = f'out/dr{shape[0]}dc{shape[1]}r{r}sd{sd}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    state = next_random_state()\n",
        "    row_dimension, col_dimension = shape\n",
        "    bl = state.rand(row_dimension, r)\n",
        "    br = state.rand(col_dimension, r)\n",
        "    b = bl @ br.T\n",
        "\n",
        "    state = next_random_state()\n",
        "    pl = state.rand(row_dimension, r)\n",
        "    pr = state.rand(col_dimension, r)\n",
        "    p = pl @ pr.T\n",
        "    p = p/p.sum()\n",
        "\n",
        "    obs = []\n",
        "\n",
        "    state = next_random_state()\n",
        "    counts = state.multinomial(num_samples, p.flatten())\n",
        "\n",
        "    for index, count in enumerate(counts):\n",
        "        # if count == 0:\n",
        "        #     print('count is 0')\n",
        "        for _ in range(count):\n",
        "            row_ind = int(index // col_dimension)\n",
        "            col_ind = int(index % col_dimension)\n",
        "            obs.append((row_ind, col_ind, b[row_ind][col_ind] + state.normal(scale=sd)))\n",
        "\n",
        "    # row_idx = list(npr.randint(row_dimension, size=num_samples))\n",
        "    # col_idx = list(npr.randint(col_dimension, size=num_samples))\n",
        "    # for i in range(num_samples):\n",
        "    #     obs.append((row_idx[i], col_idx[i], b[row_idx[i]][col_idx[i]] + npr.normal(scale=sd)))\n",
        "\n",
        "    ds = Dataset(obs, shape=shape)\n",
        "\n",
        "    margin_weight = ds.get_marginal_probabilities()\n",
        "\n",
        "    algorithms = {\n",
        "        # 'free': create_free(shape),\n",
        "        'margin': create_margin(shape, margin_weight),\n",
        "        'nu-recommend': create_nu_recommend(shape),\n",
        "    }\n",
        "\n",
        "    # run the algorithms\n",
        "    for name, alg in algorithms.items():\n",
        "        print('current alg is', name)\n",
        "        alphas = alg.get_alpha_seq(ds, alpha_min=0.01, num_alpha=30)\n",
        "\n",
        "        if 'nu-recommend' in name:\n",
        "            b_hat_margin = np.loadtxt(f'{output_dir}/marginseed{run_seed}run{run}obs{num_samples}.txt')\n",
        "            alg.set_weight_nuc_norm_for_nu_rec(b_hat_margin, margin_weight, ell, gamma)\n",
        "\n",
        "        bs = alg.fit(ds, alphas)\n",
        "        alg.update_metrics(alphas, bs, b, p)\n",
        "        np.savetxt(f'{output_dir}/{name}seed{run_seed}run{run}obs{num_samples}.txt', alg.metrics.get_best_b_hat(bs))\n",
        "\n",
        "    return {name: alg.metrics for name, alg in algorithms.items()}\n",
        "\n",
        "\n",
        "def run_all(seed, n_run, shape, r, sd, num_samples, ell, gamma, output=None, pattern=None):\n",
        "    aggs = {}\n",
        "\n",
        "    for run in range(n_run):\n",
        "\n",
        "        print('current run is', run)\n",
        "        run_seed = 1000 * (seed + run)\n",
        "        next_random_state = random_state_generator(run_seed)\n",
        "\n",
        "        metrics = run_once(run_seed, run, shape, r, sd, num_samples, next_random_state, ell, gamma)\n",
        "\n",
        "        for name, metric in metrics.items():\n",
        "            if name not in aggs:\n",
        "                aggs[name] = tuple(MetricAggregator() for _ in range(2))\n",
        "\n",
        "            aggs[name][0].aggregate(np.min(metric.l2_errors))\n",
        "            aggs[name][1].aggregate(np.min(metric.frobenius_errors))\n",
        "\n",
        "            if output is not None:\n",
        "                for t in metric:\n",
        "                    print(pattern.format(run=run, run_seed=run_seed, num_samples=num_samples, **t), file=output)\n",
        "\n",
        "    x = 1\n",
        "    fig = plt.figure()\n",
        "    for name, agg in aggs.items():\n",
        "        y, _, yerr = agg[0].get_confidence_band()\n",
        "        plt.errorbar(2*x, y, yerr=yerr, label=f'{name}', marker='s')\n",
        "        x += 1\n",
        "\n",
        "    plt.title('l2 error')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    fig = plt.figure()\n",
        "    for name, agg in aggs.items():\n",
        "        y, _, yerr = agg[0].get_confidence_band()\n",
        "        plt.errorbar(2 * x, y, yerr=yerr, label=f'{name}', marker='s')\n",
        "        x += 1\n",
        "\n",
        "    plt.title('frobenius error')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def __main__():\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument('--shape', nargs='+', type=int, default=(100, 100))\n",
        "    parser.add_argument('--r', type=int, default=20)\n",
        "    parser.add_argument('--sd', type=float, default=1)\n",
        "    parser.add_argument('--obs', type=int, default=2000)\n",
        "    parser.add_argument('--run', type=int, default=2)\n",
        "    parser.add_argument('--seed', type=int, default=1)\n",
        "    parser.add_argument('--ell', type=float, default=3)\n",
        "    parser.add_argument('--gamma', type=float, default=3)\n",
        "    # parser.add_argument('--name', type=str, default='output')\n",
        "    parser.add_argument('--name', type=str, default=None)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.name is not None:\n",
        "        columns = ['run', 'run_seed', 'num_samples', 'alpha', 'l2_error', 'frobenius_error']\n",
        "\n",
        "        header = ','.join(columns)\n",
        "        pattern = ','.join([f'{{{col}}}' for col in columns])\n",
        "\n",
        "        os.makedirs('out', exist_ok=True)\n",
        "\n",
        "        with open(f'out/{args.name}.csv', 'w') as output:\n",
        "            print(header, file=output)\n",
        "            run_all(args.seed, args.run, args.shape, args.r, args.sd, args.obs, args.ell, args.gamma, output, pattern)\n",
        "    else:\n",
        "        run_all(args.seed, args.run, args.shape, args.r, args.sd, args.obs, args.ell, args.gamma)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    __main__()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lab Test Data Process"
      ],
      "metadata": {
        "id": "bfIMs38Y9--D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "#  Real-lab data ▸  Margin  vs  NU-Recommend   (ℓ = 2, γ = 6)\n",
        "#  Outputs per seed\n",
        "#      • test-RMSE  (log-space)\n",
        "#      • test rel-Frobenius error\n",
        "#      • best α_margin , best α_NU\n",
        "# ===============================================================\n",
        "\n",
        "import pandas as pd, numpy as np, cvxpy as cp, time\n",
        "from base import Dataset\n",
        "from wmc  import WmcImpute\n",
        "\n",
        "RAW_CSV  = '/content/anonymized2_1Year.csv'\n",
        "SEEDS    = [0]                     # change to range(10) etc.\n",
        "VAL_FRAC = 0.10;  TEST_FRAC = 0.10\n",
        "ELL, GAMMA = 2.0, 6.0\n",
        "NUM_ALPHA  = 30\n",
        "\n",
        "# ---------- load, filter, pivot ---------------------------------\n",
        "data  = pd.read_csv(RAW_CSV)\n",
        "pat_ct= data.groupby('myPatID').size()\n",
        "lab_ct= data.groupby('myCID'  ).size()\n",
        "mask  = (pat_ct[data['myPatID']] >110)&(pat_ct[data['myPatID']]<4000)&\\\n",
        "        (lab_ct[data['myCID'  ]] >1000)\n",
        "mat_raw = ( np.log1p(\n",
        "              data.loc[mask]\n",
        "                  .pivot_table(index='myCID', columns='myPatID',\n",
        "                               values='LV', aggfunc='mean')\n",
        "           ).to_numpy(dtype='float64') )\n",
        "\n",
        "rows_all, cols_all = np.where(~np.isnan(mat_raw))\n",
        "vals_all           = mat_raw[rows_all, cols_all]\n",
        "M_fro_sq           = np.nansum(mat_raw**2)\n",
        "\n",
        "# ===============================================================\n",
        "def run_one_seed(seed):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    m   = len(vals_all)\n",
        "\n",
        "    perm      = rng.permutation(m)\n",
        "    n_test    = int(TEST_FRAC * m)\n",
        "    n_val     = int(VAL_FRAC  * m)\n",
        "\n",
        "    idx_test  = perm[:n_test]\n",
        "    idx_val   = perm[n_test : n_test+n_val]\n",
        "    idx_train = perm[n_test+n_val:]\n",
        "\n",
        "    # ----- μ,σ on TRAIN slice only -----\n",
        "    col_mu = np.nanmean(mat_raw[:, cols_all[idx_train]], axis=1)\n",
        "    col_sd = np.nanstd (mat_raw[:, cols_all[idx_train]], axis=1)\n",
        "    col_sd[col_sd==0] = 1\n",
        "    mat = (mat_raw - col_mu[:,None]) / col_sd[:,None]\n",
        "\n",
        "    def ds_from(idx):\n",
        "        return Dataset(list(zip(rows_all[idx], cols_all[idx],\n",
        "                                mat[rows_all[idx], cols_all[idx]])),\n",
        "                       mat.shape)\n",
        "\n",
        "    ds_train = ds_from(idx_train)\n",
        "    rv_r, rv_c = rows_all[idx_val],  cols_all[idx_val]\n",
        "    rv_v      = mat[rv_r, rv_c]\n",
        "    rt_r, rt_c= rows_all[idx_test], cols_all[idx_test]\n",
        "    rt_v_raw  = mat_raw[rt_r, rt_c]\n",
        "\n",
        "    # ---------- Margin baseline ----------\n",
        "    w_mar = ds_train.get_marginal_probabilities()\n",
        "    mar   = WmcImpute(mat.shape, weight_nuc_norm=w_mar)\n",
        "    alpha_seq = mar.get_alpha_seq(ds_train, 0.01, NUM_ALPHA)\n",
        "    B_mar_path = mar.fit(ds_train, alpha_seq)\n",
        "    rmse_val = [np.sqrt(np.mean((B[rv_r,rv_c]-rv_v)**2)) for B in B_mar_path]\n",
        "    best_i   = int(np.argmin(rmse_val))\n",
        "    B_mar    = B_mar_path[best_i]\n",
        "    alpha_mar_best = alpha_seq[best_i]\n",
        "\n",
        "    # ---------- NU-Recommend ----------\n",
        "    nu = WmcImpute(mat.shape)\n",
        "    nu.set_weight_nuc_norm_for_nu_rec(B_mar, w_mar, ell=ELL, gamma=GAMMA)\n",
        "    B_nu_path = nu.fit(ds_train, alpha_seq)\n",
        "    rmse_val_nu = [np.sqrt(np.mean((B[rv_r,rv_c]-rv_v)**2)) for B in B_nu_path]\n",
        "    best_j   = int(np.argmin(rmse_val_nu))\n",
        "    B_nu     = B_nu_path[best_j]\n",
        "    alpha_nu_best = alpha_seq[best_j]\n",
        "\n",
        "    # ---------- re-scale and score ----------\n",
        "    def score(B_z):\n",
        "        B_log = B_z*col_sd[:,None] + col_mu[:,None]\n",
        "        rmse  = np.sqrt(np.mean((B_log[rt_r,rt_c]-rt_v_raw)**2))\n",
        "        relF  = np.sqrt(np.sum((B_log[rt_r,rt_c]-rt_v_raw)**2)) / np.sqrt(M_fro_sq)\n",
        "        return rmse, relF\n",
        "\n",
        "    return score(B_mar), score(B_nu), alpha_mar_best, alpha_nu_best\n",
        "\n",
        "# ===============================================================\n",
        "print(\"seed | RMSE_m  RMSE_nu | relF_m relF_nu | α_m  α_nu\")\n",
        "for s in SEEDS:\n",
        "    (rm_m,rf_m),(rm_nu,rf_nu),a_m,a_nu = run_one_seed(s)\n",
        "    print(f\"{s:4} | {rm_m:6.4f}  {rm_nu:6.4f} | {rf_m:.4f} {rf_nu:.4f} | {a_m:.3g} {a_nu:.3g}\")\n"
      ],
      "metadata": {
        "id": "UoAjz5LrqtnY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}